---
title: "Random Forest (RF)"
author: "Feature space partition ensemble model for replication (FESPAE)"
date: "Simulation-based settings"
output:
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**Article:** Lizbeth Naranjo, Carlos J. Perez, Daniel F. Merino (2025). 
A data ensemble-based approach for detecting vocal disorders using replicated acoustic biomarkers from electroglottography.  
*Sensing and Bio-Sensing Research Journal*, vol, num, pages. 

```{r, eval=TRUE, include=TRUE, message=FALSE, warning=FALSE, echo=FALSE, cache=FALSE, results=TRUE,  comment=""}   
library(tidyverse) 
library(randomForest)
## change the address where the file will be saved 
address = "~/Documents/GitHub/" 
setwd("~/Documents/GitHub/") 
```

# Data 

```{r, eval=TRUE, include=TRUE, message=FALSE, warning=FALSE, echo=TRUE, cache=FALSE, results=TRUE,  comment="", out.width="100%"}  
## read data
datos2 <- read.csv(paste0(address,"data_simulated.csv"),
                   sep = ";",header=TRUE, dec=".")

archivo = "FESPAE_crossval_strata_RF_simula"   ## name of the files to save results
```

```{r, eval=TRUE, include=TRUE, message=FALSE, warning=FALSE, echo=TRUE, cache=FALSE, results=TRUE,  comment="", out.width="100%"}  
dim(datos2)
summary(datos2)
head(datos2)

datos2 <- as.data.frame(datos2)
datos2$ID_fact = as.factor(datos2$ID)   ## categorical ID of the subject
datos2$STATUS_fact = as.factor(datos2$status)   ## categorical response variable
table(datos2$STATUS_fact)

## data set 
trainc <- datos2 %>% select(-status,-ID) 
```

\clearpage 
# Crossvalidation 

## Subspaces

```{r, echo=TRUE, eval=TRUE, fig.show="hold", include=TRUE, out.width="100%", warning=FALSE, message=FALSE} 
## Partition of subspaces
## The feature space is randomly partitioned into K subspaces with roughly equal sizes
## k = number of predictors
## K = subspaces 

K0 = 3 ### sub-spaces
k = 21 ### explanatory variable 
k2 = round(k/K0)
space = 1:k
subspaces = rep(list(rep(NA,k2)),K0)
set.seed(12345)
for(j in 1:(K0-1)){
    space1 = sample(space, size=k2, replace=FALSE)
    space = setdiff(space,space1)
    subspaces[[j]] = space1[order(space1)]
}
space1 = space
subspaces[[K0]] = space1[order(space1)]
# 21 features = 1x21, 3x7, 
subspaces
``` 

\clearpage 
## Training and testing data subsets  

```{r, echo=TRUE, eval=TRUE, fig.show="hold", include=TRUE, out.width="100%", warning=FALSE, message=FALSE} 
## Select data: 75% training & 25% testing stratified per category
SIM = 100   ## repeat N times the cross-validation process
N = 300   ## sample size 
Nfit = 225   ## sample size for training subset
Ntest = 75   ## sample size for testing subset
Ncat = 100   ## sample size per category
Ncatfit = 75   ## training per category 
Ncattest = 25   ## testing per category
FIT <- matrix(0,SIM,Nfit)   ## training subsets 
TEST <- matrix(0,SIM,Ntest)   ## testing subsets 

categoria = trainc %>% filter(rep==1) %>% select(STATUS_fact)
categoria = as.numeric(categoria$STATUS_fact)
id = 1:N
set.seed(12345)
for(si in 1:SIM){
  for(j in 1:3){ 
    idcat = id[categoria==j]   ## stratified per category j
    ran0 = sample(idcat, size=Ncatfit, replace=FALSE)
    
    FIT[si,(j-1)*Ncatfit+1:Ncatfit] <- sort(ran0)
    TEST[si,(j-1)*Ncattest+1:Ncattest] <- setdiff(idcat,ran0)
} }
``` 

\newpage 
## Classification metrics for models predicting nominal outcomes

```{r, echo=TRUE, eval=TRUE, fig.show="hold", include=TRUE, out.width="100%", warning=FALSE, message=FALSE} 
## Functions to compute classification metrics 
## Ytrue = true response variable
## Ypred = predicted outcome
## cat = category 
## TP = true positive 
## TN = true negative
## FP = false positive 
## FN = false negative 

## Function to compute the precision per class=cat
fn_precision_class <- function(Ytrue,Ypred,cat){ 
  TP = sum(Ypred[Ytrue==cat]==cat)
  FP = sum(Ypred[Ytrue!=cat]==cat)
  precision = TP/(TP+FP)
  return(precision)
}

## Function to compute the recall per class=cat
fn_recall_class <- function(Ytrue,Ypred,cat){ ## cat==category
  TP = sum(Ypred[Ytrue==cat]==cat)
  FN = sum(Ypred[Ytrue==cat]!=cat)
  recall = TP/(TP+FN)  
  return(recall)
}

## Function to compute the F1-score per class=cat
fn_f1score_class <- function(Ytrue,Ypred,cat){ ## cat==category
  TP = sum(Ypred[Ytrue==cat]==cat)
  FP = sum(Ypred[Ytrue!=cat]==cat)
  FN = sum(Ypred[Ytrue==cat]!=cat)
  precision = TP/(TP+FP)
  recall = TP/(TP+FN)  
  f1score = 2*(precision*recall)/(precision+recall)
  return(f1score)
}

## To save classification metrics 
## Fitxxx: metric for training subset. Testxxx: metric for testing subset
FitAccuracy = TestAccuracy <- array(NA,dim=c(SIM,4))  ## Accuracy Rate
FitPrecisionClass = TestPrecisionClass <- array(NA,dim=c(SIM,4,3))  ## Precision per class
FitRecallClass = TestRecallClass <- array(NA,dim=c(SIM,4,3))  ## Recall per class
FitF1ScoreClass = TestF1ScoreClass <- array(NA,dim=c(SIM,4,3))  ## F1-score per class
FitPrecisionMacroAve = TestPrecisionMacroAve <- array(NA,dim=c(SIM,4))  ## Precision Macro Average
FitRecallMacroAve = TestRecallMacroAve <- array(NA,dim=c(SIM,4))  ## Recall Macro Average
FitF1ScoreMacroAve = TestF1ScoreMacroAve <- array(NA,dim=c(SIM,4))  ## F1-score Macro Average 
```

\newpage
## Model estimation  

```{r, echo=TRUE, eval=TRUE, fig.show="hold", include=TRUE, out.width="100%", warning=FALSE, message=FALSE, error=TRUE} 
##--------------------------------------------------
for(sim in 1:SIM){ ### BEGIN sim
##--------------------------------------------------
    
my_fit = FIT[sim,]   ## training subset 
my_test = TEST[sim,]   ## testing subset 

## Training data subset 
train1 <- trainc %>% filter(ID_fact%in%my_fit, rep==1)  ## repetition=1
train2 <- trainc %>% filter(ID_fact%in%my_fit, rep==2)  ## repetition=2 
train3 <- trainc %>% filter(ID_fact%in%my_fit, rep==3)  ## repetition=3 

Yc = train1$STATUS_fact   ## categorical response variable for training
n = length(Yc)  
G = 3 # classes

## Testing data subset
test1 <- trainc %>% filter(ID_fact%in%my_test, rep==1)  ## repetition=1
test2 <- trainc %>% filter(ID_fact%in%my_test, rep==2)  ## repetition=2
test3 <- trainc %>% filter(ID_fact%in%my_test, rep==3)  ## repetition=3

Yc.new = test1$STATUS_fact   ## categorical response variable for testing 
n.new = length(Yc.new)

## Delete variables which are not used 
train1 <- train1 %>% select(-c(rep,ID_fact))
train2 <- train2 %>% select(-c(rep,ID_fact))
train3 <- train3 %>% select(-c(rep,ID_fact))
test1 <- test1 %>% select(-c(rep,ID_fact))
test2 <- test2 %>% select(-c(rep,ID_fact))
test3 <- test3 %>% select(-c(rep,ID_fact))

##--------------------------------------------------
## Algorithm FESPAE 
## Feature space partition ensemble model for replication 
##--------------------------------------------------

## Algo1: The feature space is randomly partitioned into M subspaces, {S1,S2,...,SM}

K0 = 3  ## sub-spaces
k = 21  ## explanatory variables  
k2 = round(k/K0)
space = 1:k
subspaces = rep(list(rep(NA,k2)),K0)  ## Subspaces 
set.seed(12345)
for(j in 1:(K0-1)){
    space1 = sample(space, size=k2, replace=FALSE)
    space = setdiff(space,space1)
    subspaces[[j]] = space1[order(space1)]
}
space1 = space
subspaces[[K0]] = space1[order(space1)]
# 21 features = 1x21, 3x7, 

##--------------------------------------------------
## Algo2: for feature subspace m = 1 to M do

pred.vgam = array(NA,dim=c(n,G,K0,3))  ## 3 repetitions
pred.new.vgam = array(NA,dim=c(n.new,G,K0,3))  ## 3 repetitions
##--------------------------------------------------
## Algo3: for replication j = 1 to J do 

## REPLICATION j=1: 
for(parti1 in 1:K0){  ## partition of the subspaces
train1_par = train1[,c(subspaces[[parti1]])] 
test1_par = test1[,c(subspaces[[parti1]])]   

## Algo4: Fit a classifier $T(xj,z)$, $xj\in Sm$, to the training data 
mod1 <- randomForest( 
  x = train1_par ,
  y = Yc , 
  ntree = 500 , 
  xtest = test1_par )
## summary(mod1)

## Algo5: Compute the C response probabilities {\pi^(m,j)_{ic}}, for i=1,...,n.  
## Predictions
predict1.vgam <- mod1$votes
predict1.new.vgam <- mod1$test$votes

pred.vgam[,,parti1,1] = predict1.vgam
pred.new.vgam[,,parti1,1] = predict1.new.vgam 
}

## REPLICATION j=2: 
for(parti2 in 1:K0){  ## partition of the subspaces
train2_par = train2[,c(subspaces[[parti2]])] 
test2_par = test2[,c(subspaces[[parti2]])]   

## Algo4: Fit a classifier $T(xj,z)$, $xj\in Sm$, to the training data 
mod2 <- randomForest( 
  x = train2_par ,
  y = Yc , 
  ntree = 500 , 
  xtest = test2_par )
## summary(mod2)

## Algo5: Compute the C response probabilities {\pi^(m,j)_{ic}}, for i=1,...,n.  
## Predictions
predict2.vgam <- mod2$votes
predict2.new.vgam <- mod2$test$votes

pred.vgam[,,parti2,2] = predict2.vgam
pred.new.vgam[,,parti2,2] = predict2.new.vgam 
}

## REPLICATION j=3: 
for(parti3 in 1:K0){  ## partition of the subspaces  
train3_par = train3[,c(subspaces[[parti3]])] 
test3_par = test3[,c(subspaces[[parti3]])]   

## Algo4: Fit a classifier $T(xj,z)$, $xj\in Sm$, to the training data 
mod3 <- randomForest( 
  x = train3_par ,
  y = Yc , 
  ntree = 500 , 
  xtest = test3_par )
## summary(mod3)

## Algo5: Compute the C response probabilities {\pi^(m,j)_{ic}}, for i=1,...,n.  
## Predictions
predict3.vgam <- mod3$votes
predict3.new.vgam <- mod3$test$votes

pred.vgam[,,parti3,3] = predict3.vgam
pred.new.vgam[,,parti3,3] = predict3.new.vgam 
} 

##--------------------------------------------------
## Algo6: End for replication j = 1 to J 
## Algo7: End for feature subspace m = 1 to M
##--------------------------------------------------
## Algo8: Output: compute the response probabilities $\pi_{ic} = mean({\pi^(m,j)_{ic}})

pred.ave1 = apply(pred.vgam[,,,1],c(1,2),mean) 
pred.ave2 = apply(pred.vgam[,,,2],c(1,2),mean) 
pred.ave3 = apply(pred.vgam[,,,3],c(1,2),mean) 

pred.ave.vgam = apply(pred.vgam,c(1,2),mean) 

### Predict new subjects 
pred.ave.new1 = apply(pred.new.vgam[,,,1],c(1,2),mean) 
pred.ave.new2 = apply(pred.new.vgam[,,,2],c(1,2),mean) 
pred.ave.new3 = apply(pred.new.vgam[,,,3],c(1,2),mean) 

pred.ave.new.vgam = apply(pred.new.vgam,c(1,2),mean) 

##--------------------------------------------------
## Algo8: Output: compute the response category T*(x,z) = arg max {\pi_{ic}}

pred.max1 <- apply(pred.ave1, 1, which.max)
pred.max2 <- apply(pred.ave2, 1, which.max)
pred.max3 <- apply(pred.ave3, 1, which.max)

pred.vgam_max <- apply(pred.ave.vgam, 1, which.max)

### Predict new subjects 

pred.new.max1 <- apply(pred.ave.new1, 1, which.max)
pred.new.max2 <- apply(pred.ave.new2, 1, which.max)
pred.new.max3 <- apply(pred.ave.new3, 1, which.max)

pred.new.vgam_max <- apply(pred.ave.new.vgam, 1, which.max)

##--------------------------------------------------
## End FESPAE
##-------------------------------------------------- 
## Classification Metrics for models predicting nominal outcomes

## Accuracy Rate 
FitAccuracy[sim,] = c(sum(Yc==pred.max1)/n, 
                      sum(Yc==pred.max2)/n, 
                      sum(Yc==pred.max3)/n, 
                      sum(Yc==pred.vgam_max)/n) 

TestAccuracy[sim,] = c(sum(Yc.new==pred.new.max1)/n.new, 
                       sum(Yc.new==pred.new.max2)/n.new, 
                       sum(Yc.new==pred.new.max3)/n.new, 
                       sum(Yc.new==pred.new.vgam_max)/n.new) 

## Precision
for(cate in 1:3){
  FitPrecisionClass[sim,1, cate] = fn_precision_class(Yc, pred.max1, cate)
  FitPrecisionClass[sim,2, cate] = fn_precision_class(Yc, pred.max2, cate)
  FitPrecisionClass[sim,3, cate] = fn_precision_class(Yc, pred.max3, cate)
  FitPrecisionClass[sim,4, cate] = fn_precision_class(Yc, pred.vgam_max, cate)

  TestPrecisionClass[sim,1, cate] = fn_precision_class(Yc.new, pred.new.max1, cate)
  TestPrecisionClass[sim,2, cate] = fn_precision_class(Yc.new, pred.new.max2, cate)
  TestPrecisionClass[sim,3, cate] = fn_precision_class(Yc.new, pred.new.max3, cate)
  TestPrecisionClass[sim,4, cate] = fn_precision_class(Yc.new, pred.new.vgam_max, cate)
}
for(rep in 1:4){
  FitPrecisionMacroAve[sim, rep] = mean(FitPrecisionClass[sim, rep,])
  TestPrecisionMacroAve[sim,rep] = mean(TestPrecisionClass[sim,rep,])
} 

## Recall
for(cate in 1:3){
  FitRecallClass[sim,1, cate] = fn_recall_class(Yc, pred.max1, cate)
  FitRecallClass[sim,2, cate] = fn_recall_class(Yc, pred.max2, cate)
  FitRecallClass[sim,3, cate] = fn_recall_class(Yc, pred.max3, cate)
  FitRecallClass[sim,4, cate] = fn_recall_class(Yc, pred.vgam_max, cate)
  
  TestRecallClass[sim,1, cate] = fn_recall_class(Yc.new, pred.new.max1, cate)
  TestRecallClass[sim,2, cate] = fn_recall_class(Yc.new, pred.new.max2, cate)
  TestRecallClass[sim,3, cate] = fn_recall_class(Yc.new, pred.new.max3, cate)
  TestRecallClass[sim,4, cate] = fn_recall_class(Yc.new, pred.new.vgam_max, cate)
}
for(rep in 1:4){
  FitRecallMacroAve[sim, rep] = mean(FitRecallClass[sim, rep,])
  TestRecallMacroAve[sim,rep] = mean(TestRecallClass[sim,rep,])
}

## F1-Score
for(cate in 1:3){
  FitF1ScoreClass[sim,1, cate]= fn_f1score_class(Yc, pred.max1, cate)
  FitF1ScoreClass[sim,2, cate]= fn_f1score_class(Yc, pred.max2, cate)
  FitF1ScoreClass[sim,3, cate]= fn_f1score_class(Yc, pred.max3, cate)
  FitF1ScoreClass[sim,4, cate]= fn_f1score_class(Yc, pred.vgam_max, cate)

  TestF1ScoreClass[sim,1, cate] = fn_f1score_class(Yc.new, pred.new.max1, cate)
  TestF1ScoreClass[sim,2, cate] = fn_f1score_class(Yc.new, pred.new.max2, cate)
  TestF1ScoreClass[sim,3, cate] = fn_f1score_class(Yc.new, pred.new.max3, cate)
  TestF1ScoreClass[sim,4, cate] = fn_f1score_class(Yc.new, pred.new.vgam_max, cate)
}
for(rep in 1:4){
  FitF1ScoreMacroAve[sim, rep] = mean(FitF1ScoreClass[sim, rep,]) 
  TestF1ScoreMacroAve[sim,rep] = mean(TestF1ScoreClass[sim,rep,]) 
}

##--------------------------------------------------
} ## END sim 
##-------------------------------------------------- 
```

\newpage
# Results 

## Accuracy Rate 

```{r, echo=TRUE, eval=TRUE, fig.show="hold", include=TRUE, out.width="100%", warning=FALSE, message=FALSE} 
columna = c("rep1","rep2","rep3","ensemble")
renglon = c("fit_mean","fit_sd","test_mean","test_sd")

summary(FitAccuracy) 
apply(FitAccuracy,2,"sd")
summary(TestAccuracy)
apply(TestAccuracy,2,"sd")

RESaccuracy <- rbind(apply(FitAccuracy,2,"mean"), apply(FitAccuracy,2,"sd"),
                     apply(TestAccuracy,2,"mean"),apply(TestAccuracy,2,"sd"))
colnames(RESaccuracy) = columna
rownames(RESaccuracy) = renglon
write.csv(RESaccuracy, file=paste0(archivo,"_accuracy",".csv"))
```

\newpage
## Precision Macro Average

```{r, echo=TRUE, eval=TRUE, fig.show="hold", include=TRUE, out.width="100%", warning=FALSE, message=FALSE} 
summary(FitPrecisionMacroAve) 
apply(FitPrecisionMacroAve,2,"sd")
summary(TestPrecisionMacroAve)
apply(TestPrecisionMacroAve,2,"sd")

RESprecision <- rbind(apply(FitPrecisionMacroAve,2,"mean"), apply(FitPrecisionMacroAve,2,"sd"),
                      apply(TestPrecisionMacroAve,2,"mean"),apply(TestPrecisionMacroAve,2,"sd"))
colnames(RESprecision) = columna
rownames(RESprecision) = renglon
write.csv(RESprecision, file=paste0(archivo,"_precision",".csv"))
```

\newpage
## Recall Macro Average

```{r, echo=TRUE, eval=TRUE, fig.show="hold", include=TRUE, out.width="100%", warning=FALSE, message=FALSE} 
summary(FitRecallMacroAve) 
apply(FitRecallMacroAve,2,"sd")
summary(TestRecallMacroAve)
apply(TestRecallMacroAve,2,"sd")

RESrecall <- rbind(apply(FitRecallMacroAve,2,"mean"), apply(FitRecallMacroAve,2,"sd"),
                   apply(TestRecallMacroAve,2,"mean"),apply(TestRecallMacroAve,2,"sd"))
colnames(RESrecall) = columna
rownames(RESrecall) = renglon
write.csv(RESrecall, file=paste0(archivo,"_recall",".csv"))
```

\newpage
## F1-Score Macro Average

```{r, echo=TRUE, eval=TRUE, fig.show="hold", include=TRUE, out.width="100%", warning=FALSE, message=FALSE} 
summary(FitF1ScoreMacroAve) 
apply(FitF1ScoreMacroAve,2,"sd")
summary(TestF1ScoreMacroAve)
apply(TestF1ScoreMacroAve,2,"sd")

RESf1score <- rbind(apply(FitF1ScoreMacroAve,2,"mean"), apply(FitF1ScoreMacroAve,2,"sd"),
                    apply(TestF1ScoreMacroAve,2,"mean"),apply(TestF1ScoreMacroAve,2,"sd"))
colnames(RESf1score) = columna
rownames(RESf1score) = renglon
write.csv(RESf1score, file=paste0(archivo,"_f1score",".csv"))
``` 

